{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wFX7JatgCPA"
   },
   "source": [
    "#MP4\n",
    "\n",
    "In this assignment you will be performing Semantic Segmentation. We've provided the dataset and some helper code to guide you along.\n",
    "\n",
    "Reminders:\n",
    "- When first getting your code to run do not use GPU as this will exhaust your colab resources\n",
    "- When you're ready to properly test your models, make sure you are connected to a GPU runtime as this does significantly speeds up execution\n",
    "    - To change your runtime do: **Runtime** --> **Change runtime type** --> under **Hardware accelerator** select **GPU**\n",
    "    - Note that changing runtime resets your kernel (meaning you will need to rerun cells and local variables will be lost)\n",
    "    - It also sets this new runtime as the default when you return to this notebook later\n",
    "- Do not start last minute, these models do take some time to train\n",
    "- Loading the data takes some time, you should only have to do this once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eIFcdyhhD1w"
   },
   "source": [
    "## Accessing the data\n",
    "\n",
    "There are multiple ways to work with data in colab.\n",
    "See this [Colab notebook](https://colab.research.google.com/notebooks/io.ipynb) or this [StackOverflow post](https://stackoverflow.com/questions/48376580/google-colab-how-to-read-data-from-my-google-drive) for more details.\n",
    "\n",
    "Once you've mounted your drive you can see your entire drive file structure by clicking the \"Files\" tab on the left.\n",
    "\n",
    "**If you wish to work locally you can ignore the first two cells, but you will still need to set the appropriate path for your dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2uohksRc8R6"
   },
   "outputs": [],
   "source": [
    "# Note there are other methods to do this\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLFBEMfVdHId"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/content/gdrive/My Drive/CS_498_MP4\"):\n",
    "    os.makedirs(\"/content/gdrive/My Drive/CS_498_MP4\")\n",
    "os.chdir(\"/content/gdrive/My Drive/CS_498_MP4\")\n",
    "\n",
    "# TODO: make sure to specify the right dataset path here\n",
    "DATASET_PATH = 'data/sbd/'\n",
    "# DATASET_PATH = '/content/gdrive/MyDrive/CS_498_MP4/data/sbd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UzpXeiVddIdi"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEJOJqm5W-KT"
   },
   "source": [
    "## Dataset (Q1)\n",
    "\n",
    "Here we define a class (pytorch Dataset) for accessing data. This allows us to perform transformations on the data (data augmentation) as we access it. Pytorch dataloaders take in a dataset and conventiently deal with the overhead of looping through it in batches. Creating such datasets/loaders significantly simplifies our training code later on.\n",
    "\n",
    "**PDF: In your pdf visualize the same image (your choice which) a couple times to demonstrate your transformations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTlJmzl7dR4_"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Data loader for the Segmentation Dataset. If data loading is a bottleneck, \n",
    "    you may want to optimize this in for faster training. Possibilities include\n",
    "    preloading all images and annotations into memory before training, to limit delays due to disk reads.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"train\", preload=True, data_dir=DATASET_PATH, transform=False):\n",
    "        assert(split in [\"train\", \"val\", \"test\"])\n",
    "        self.img_dir = os.path.join(data_dir, split)\n",
    "        self.classes = []\n",
    "        with open(os.path.join(data_dir, 'classes.txt'), 'r') as f:\n",
    "            for l in f:\n",
    "                self.classes.append(l.rstrip())\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.split = split\n",
    "        self.data = glob.glob(self.img_dir + '/*.jpg') \n",
    "        self.data = [os.path.splitext(l)[0] for l in self.data]\n",
    "        self.transform = transform\n",
    "        self.preload = preload\n",
    "        # preload data\n",
    "        if preload:\n",
    "            self.images = [Image.open(self.data[index] + '.jpg') for index in range(len(self.data))]\n",
    "            self.ground_truth = [Image.open(self.data[index] + '.png') for index in range(len(self.data))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.preload:\n",
    "            img = self.images[index]\n",
    "            gt = self.ground_truth[index]\n",
    "        else:\n",
    "            img = Image.open(self.data[index] + '.jpg')\n",
    "            gt = Image.open(self.data[index] + '.png')\n",
    "\n",
    "        # Question 1: data augmentation\n",
    "        # hint: how does transforming the image affect the ground truth?\n",
    "        # Note: your transformation should not change the image dimensions    \n",
    "        if self.transform:\n",
    "            # Your code\n",
    "            # -------------------------\n",
    "            pass\n",
    "            # -------------------------\n",
    "\n",
    "        img = ToTensor()(img)\n",
    "        gt = torch.LongTensor(np.array(gt)).unsqueeze(0)\n",
    "\n",
    "        return img, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQYG-4XptQCa"
   },
   "outputs": [],
   "source": [
    "dataset = SegmentationDataset(split=\"train\", preload=True, data_dir=DATASET_PATH, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjYE1HCgHJQU"
   },
   "outputs": [],
   "source": [
    "# TODO: set the batch size, when running experiments later you should try different batch sizes\n",
    "training_batch_size = None\n",
    "dataloader = data.DataLoader(dataset, batch_size=training_batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INQjI7ttWbkR"
   },
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataset(split=\"val\", preload=True, data_dir=DATASET_PATH, transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2KtQmTeZKUA"
   },
   "outputs": [],
   "source": [
    "val_dataloader = data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE7Ul41tznGe"
   },
   "outputs": [],
   "source": [
    "def view_image(idx):\n",
    "    img = dataset[idx]\n",
    "    _, axes = plt.subplots(1,2)\n",
    "    axes[0].imshow(np.swapaxes(np.swapaxes(img[0], 0, 2), 0, 1))\n",
    "    axes[1].imshow(img[1][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39zXUKoZz2xZ"
   },
   "outputs": [],
   "source": [
    "# You might want to look at a bunch of different images to get a feel for your data\n",
    "view_image(random.randint(0, len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aibsLvHMXFxS"
   },
   "source": [
    "## Simple Baseline (Q2)\n",
    "\n",
    "This is a trivial semantic segmentor. For each pixel location it computes the \n",
    "distribution of the class label in the training set and uses that as the \n",
    "prediction. In other words, if a pixel is \"sky\" half the time and \"water\" the other half in the training data, you should label it as [0.5,0,0,0,0.5,0,0,0,0].\n",
    "\n",
    "**PDF: in your pdf report the evaluation metrics (from the next question) for this simple baseline. Also visualize the output image of simple_predict (since simple_predict outputs the same segmentation regardless of input you can just report a single image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okJ-P_JGesKq"
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "# Output shape: (num_classes, 224, 288)\n",
    "def simple_train(num_classes, train_dataloader):\n",
    "    # Your code\n",
    "    # -------------------------\n",
    "    model = np.ones((num_classes, 224, 288)) / num_classes\n",
    "    # -------------------------\n",
    "    return model\n",
    "\n",
    "# Output:\n",
    "#   gt: the ground truth segmentation, shape (dataset_size, 1, 224, 288)\n",
    "#   preds: the predicted segmentation class probabilities, shape (dataset_size, 9, 224, 288) \n",
    "def simple_predict(dataloader, model):\n",
    "    gts, preds = [], []\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        # Your code\n",
    "        # -------------------------\n",
    "        pass\n",
    "        # -------------------------\n",
    "    return np.array(gts), np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLa3o95oHT3Z"
   },
   "outputs": [],
   "source": [
    "# our \"model\" is class frequency, train it then make predictions for the validation set \n",
    "class_freq = simple_train(dataset.n_classes, dataloader)\n",
    "gts, preds = simple_predict(val_dataloader, class_freq)\n",
    "classes = list(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT_v0FPEPOcs"
   },
   "outputs": [],
   "source": [
    "# visualize the output segmentation prediction\n",
    "plt.imshow(np.argmax(preds[0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6s7jES4XB-C"
   },
   "source": [
    "## Evaluation Metrics (Q3)\n",
    "\n",
    "We've implemented mean average precision. Your job is to compute the confusion matrix and IoU for a set of predictions. Namely, fill in the compute_confusion_matrix function.\n",
    "\n",
    "The **(i,j)**th entry of a confusion matrix computes the number of observations known to be in group **i** and predicted to be in group **j**. You can use [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) as a reference and sanity check.\n",
    "\n",
    "IoU is the intersection of the predicted and ground truth segmentation masks divided by their union. Think how these values are related to what you've already computed in the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6Dtn7yudjC8"
   },
   "outputs": [],
   "source": [
    "def segmentation_eval(gts, preds, classes, plot_file_name):\n",
    "    \"\"\"\n",
    "    @param    gts               numpy.ndarray   ground truth labels\n",
    "    @param    preds             numpy.ndarray   predicted labels\n",
    "    @param    classes           string          class names\n",
    "    @param    plot_file_name    string          plot file names\n",
    "    \"\"\"\n",
    "    ious, counts = compute_confusion_matrix(gts, preds)\n",
    "    aps = compute_ap(gts, preds)\n",
    "    plot_results(counts, ious, aps, classes, plot_file_name)\n",
    "    for i in range(len(classes)):\n",
    "        print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format(classes[i], aps[i], ious[i]))\n",
    "    print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format('mean', np.mean(aps), np.mean(ious)))\n",
    "    return aps, ious\n",
    "\n",
    "def plot_results(counts, ious, aps, classes, file_name):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    conf = counts / np.sum(counts, 1, keepdims=True)\n",
    "    conf = np.concatenate([conf, np.array(aps).reshape(-1,1), \n",
    "                           np.array(ious).reshape(-1,1)], 1)\n",
    "    conf = conf * 100.\n",
    "    sns.heatmap(conf, annot=True, ax=ax, fmt='3.0f') \n",
    "    arts = [] \n",
    "    # labels, title and ticks\n",
    "    _ = ax.set_xlabel('Predicted labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_ylabel('True labels')\n",
    "    arts.append(_)\n",
    "    _ = ax.set_title('Confusion Matrix, mAP: {:5.1f}, mIoU: {:5.1f}'.format(\n",
    "      np.mean(aps)*100., np.mean(ious)*100.))\n",
    "    arts.append(_)\n",
    "    _ = ax.xaxis.set_ticklabels(classes + ['AP', 'IoU'], rotation=90)\n",
    "    arts.append(_)\n",
    "    _ = ax.yaxis.set_ticklabels(classes, rotation=0)\n",
    "    arts.append(_)\n",
    "    # fig.savefig(file_name, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def compute_ap(gts, preds):\n",
    "    aps = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        ap, prec, rec = calc_pr(gts == i, preds[:,i:i+1,:,:])\n",
    "        aps.append(ap)\n",
    "    return aps\n",
    "\n",
    "def calc_pr(gt, out, wt=None):\n",
    "    gt = gt.astype(np.float64).reshape((-1,1))\n",
    "    out = out.astype(np.float64).reshape((-1,1))\n",
    "    tog = np.concatenate([gt, out], axis=1)*1.\n",
    "    ind = np.argsort(tog[:,1], axis=0)[::-1]\n",
    "    tog = tog[ind,:]\n",
    "    cumsumsortgt = np.cumsum(tog[:,0])\n",
    "    cumsumsortwt = np.cumsum(tog[:,0]-tog[:,0]+1)\n",
    "    prec = cumsumsortgt / cumsumsortwt\n",
    "    rec = cumsumsortgt / np.sum(tog[:,0])\n",
    "    ap = voc_ap(rec, prec)\n",
    "    return ap, rec, prec\n",
    "\n",
    "def voc_ap(rec, prec):\n",
    "    rec = rec.reshape((-1,1))\n",
    "    prec = prec.reshape((-1,1))\n",
    "    z = np.zeros((1,1)) \n",
    "    o = np.ones((1,1))\n",
    "    mrec = np.vstack((z, rec, o))\n",
    "    mpre = np.vstack((z, prec, z))\n",
    "\n",
    "    mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n",
    "    I = np.where(mrec[1:] != mrec[0:-1])[0]+1;\n",
    "    ap = np.sum((mrec[I] - mrec[I-1])*mpre[I])\n",
    "    return ap\n",
    "    \n",
    "# Question 3: compute the confusion matrix and IoU metrics for each class\n",
    "# Hint: once you've computed the confusion matrix, IoU is easy\n",
    "# Note: preds contains class probabilities, convert this to a class prediction\n",
    "def compute_confusion_matrix(gts, preds):\n",
    "    # Your code\n",
    "    IoU = np.zeros((9))\n",
    "    conf = np.eye(9)\n",
    "    return IoU, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2fZb5RExOey"
   },
   "outputs": [],
   "source": [
    "# Evaluate our trivial segmentor\n",
    "aps, ious = segmentation_eval(gts, preds, classes, 'cs543-simple-train.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH7d_f3B5c6z"
   },
   "source": [
    "## Loss function (Q4)\n",
    "\n",
    "Implement the weighted cross entropy loss. \n",
    "\n",
    "You may not call nn.CrossEntropy but can use it as a good reference and sanity check.\n",
    "\n",
    "**PDF: in your pdf please describe the cross entropy loss. Also explain the purpose of using a weighted loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opOvrt9O5qf-"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_criterion(predictions, labels, weights):\n",
    "    # your code\n",
    "    loss = 0\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aim5asPbXJ_U"
   },
   "source": [
    "## Training loop (Q5)\n",
    "\n",
    "Fill in the training loop. We've provided validation code as well as skeleton code for training.\n",
    "\n",
    "Keep in mind that you need to move data onto the device (GPU) as you cycle through the dataloader\n",
    "\n",
    "While we've provided you with a skeleton to fill in, you should feel free to modify the visualization code for debugging purposes. For example you might want to print out the loss each iteration instead of once per epoch. Or you might want to compute validation accuracy metrics (like IoU) instead of just validation loss.\n",
    "\n",
    "**PDF: in your pdf please describe why it is important to consider both validation and training losses simultaneously. When loss stops decreasing, can we change something about the training parameters to continue improving the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sJiJPb3huXK"
   },
   "outputs": [],
   "source": [
    "def validate_model(val_loader, model, classes, device, show_matrix=False):\n",
    "    preds = np.array([]).reshape(0,9,224,288)\n",
    "    gts = np.array([]).reshape(0,1,224,288)\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs).cpu().numpy()\n",
    "            preds = np.concatenate([preds, outputs], axis=0)\n",
    "            gts = np.concatenate([gts, labels.numpy()], axis=0)\n",
    "            \n",
    "            print(\"Validating...{}\\r\".format(100.0*len(preds)/len(val_loader)), end=\"\")\n",
    "    \n",
    "    if show_matrix:\n",
    "        aps, ious = segmentation_eval(gts, preds, classes, 'cs543-simple-val_3.pdf')\n",
    "    else:\n",
    "        ious, counts = compute_confusion_matrix(gts, preds)\n",
    "        aps = compute_ap(gts, preds)\n",
    "        for i in range(len(classes)):\n",
    "            print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format(classes[i], aps[i], ious[i]))\n",
    "        print('{:>20s}: AP: {:0.2f}, IoU: {:0.2f}'.format('mean', np.mean(aps), np.mean(ious)))\n",
    "\n",
    "    return preds, gts\n",
    "\n",
    "# Your goal is to complete this function\n",
    "def train(model, optimizer, criterion, trainloader, device, valloader = None, epochs=15):\n",
    "    train_loss_over_epochs = []\n",
    "    val_loss_over_epochs = []\n",
    "    plt.ioff()\n",
    "    fig = plt.figure()\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        # running loss is the **average** loss for each item in the dataset during this epoch\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Your code\n",
    "            # -------------------------\n",
    "            pass\n",
    "            # -------------------------\n",
    "\n",
    "        train_loss_over_epochs.append(running_loss)\n",
    "        # Note: it can be more readable to overwrite the previous line - end=\"\\r\"\n",
    "        print('Epoch: {}, training loss: {:.3f}'.format(epoch + 1, running_loss))\n",
    "\n",
    "        # If you pass in a validation dataloader then compute the validation loss\n",
    "        if not valloader is None:\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in valloader:\n",
    "                    # Your code\n",
    "                    # -------------------------\n",
    "                    pass\n",
    "                    # -------------------------\n",
    "            val_loss_over_epochs.append(val_loss)\n",
    "            print('Epoch: {}, training loss: {:.3f}'.format(epoch + 1, val_loss))\n",
    "        \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(np.arange(epochs), train_loss_over_epochs, color='red', label='train')\n",
    "    if not valloader is None:\n",
    "        plt.plot(np.arange(epochs), val_loss_over_epochs, color='blue', label='val')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xticks(np.arange(epochs, dtype=int))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjavHluFXQQo"
   },
   "source": [
    "## Model definitions (Q6)\n",
    "\n",
    "Now create your models. Create one basic Convolutional architecture and one U-Net architecture.\n",
    "\n",
    "We provide some helpful methods below to compute the size of your next convolutional layer (you can find these formula at TODO).\n",
    "\n",
    "Some things to keep in mind:\n",
    "- your basic layer is nn.Conv2D, read its documentation\n",
    "- for UNet you will also need nn.ConvTranspose2D and Pooling layers\n",
    "- nn.BatchNorm2d is incredibly helpful between layers\n",
    "- you can stick to ReLU activations, but are welcome to report results with other activation functions\n",
    "\n",
    "**PDF: in your pdf please describe your final model architectures. Report the training plots and final accuracy metrics on the validation set for each model. What batch size, learning rate, optimizer did you find works best. Perform a small ablation study: what is the effect of batchnorm on training speed and accuracy? Visualize a few images and their predicted segmentation masks by your UNet model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivdsc7JntBKn"
   },
   "outputs": [],
   "source": [
    "def conv_out_size(inp_size, kernel_size, dilation, padding, stride):\n",
    "    return ((inp_size + 2*padding - dilation * (kernel_size - 1) - 1) // stride) + 1\n",
    "\n",
    "def conv_trans_out_size(inp_size, kernel_size, dilation, padding, stride, out_padding):\n",
    "    return (inp_size - 1) * stride - 2*padding + dilation * (kernel_size - 1) + out_padding + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8JFBS5zs7qS"
   },
   "outputs": [],
   "source": [
    "class BaseConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseConv, self).__init__()\n",
    "        \n",
    "        # Your code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oMHIGsCdckB"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Your code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZfzCe7OXXAo"
   },
   "source": [
    "### Now we can finally train our models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeZky2AHeyec"
   },
   "outputs": [],
   "source": [
    "# if runtime has GPU use GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-enlYgsAyNV"
   },
   "outputs": [],
   "source": [
    "# For the weighted cross entropy loss we can compute class weights using our simple baseline\n",
    "class_freq = simple_train(dataset.n_classes, dataloader)\n",
    "class_weights = []\n",
    "for i in range(9):\n",
    "    class_weights.append(1 / np.mean(class_freq[i, :, :]))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t63AoeXITni9"
   },
   "source": [
    "### Basic convolutional model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AHoBzsPtB-x"
   },
   "outputs": [],
   "source": [
    "# First make the model and put it on the device\n",
    "base_model = BaseConv().to(device)\n",
    "\n",
    "# Now define our loss criterion as cross entropy based on your previous code\n",
    "#criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "criterion = lambda y_pred, y_true: cross_entropy_criterion(y_pred, y_true, class_weights)\n",
    "\n",
    "# Now make our optimizer for this model\n",
    "# TODO: pick an optimizer from torch.optim and set the learning rate\n",
    "lr = None\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03o2Jn-Xuc_D"
   },
   "outputs": [],
   "source": [
    "# Now train and validate\n",
    "# Consider putting this code into a loop, \n",
    "# thus alternating between training for some number of epochs and validating\n",
    "\n",
    "# TODO: how many epochs to train for?\n",
    "epochs = None\n",
    "base_model = train(base_model, optimizer, criterion, dataloader, device, epochs=epochs)\n",
    "preds, gts = validate_model(val_dataloader, base_model, list(dataset.classes), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N0JIgMUvpg8"
   },
   "source": [
    "#### UNet model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWKLqr8Ou0y2"
   },
   "outputs": [],
   "source": [
    "model_unet = UNet().to(device)\n",
    "\n",
    "# TODO: fill in this code as you did above for the basic convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPtyNZXnz4wF"
   },
   "source": [
    "Make sure to report the results for both models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vl9LrgBXgQ5"
   },
   "source": [
    "## Working off a pretrained model (Q7)\n",
    "\n",
    "Finally, you will now modify a pretrained model (resnet18) and use it as an initialization for training. You should be able to get better results with this model than before.\n",
    "\n",
    "You can finetune (meaning backpropagate through the resnet layers) or not. You can finetune just some layers and not others. It's up to you.\n",
    "\n",
    "**PDF: in your pdf report the final accuracy of your model based on a pretrained model. Describe how you used the pretrained model, which features did you extract and why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ9v976X9pUL"
   },
   "outputs": [],
   "source": [
    "pretrained_resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZiaaE0RAjkl"
   },
   "outputs": [],
   "source": [
    "class ResnetBasedModel(nn.Module):\n",
    "    def __init__(self, pretrained_resnet, num_layers_to_remove=0):\n",
    "        super(ResnetBasedModel, self).__init__()\n",
    "        # You can, for example, extract the first N layers of the model like this:\n",
    "        # self.resnet_features = nn.Sequential(*list(pretrained_resnet.children())[:N])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqek-1wl2WGQ"
   },
   "outputs": [],
   "source": [
    "resnet_based_model = ResnetBasedModel(pretrained_resnet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TilIM8Nd5mVZ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
    "optimizer = optim.Adam(resnet_based_model.parameters(), lr=0.001)\n",
    "\n",
    "resnet_based_model = train(resnet_based_model, optimizer, criterion, dataloader, device, epochs=15)\n",
    "preds, gts = validate_model(val_dataloader, resnet_based_model, list(dataset.classes), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD4JrAOxIJva"
   },
   "source": [
    "# Test set\n",
    "\n",
    "Finally we can check evaluation on test set....\n",
    "\n",
    "**PDF: in your pdf report the results of your best model (this should be based on a pretrained model) on the test dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNVIpfRaIAwu"
   },
   "outputs": [],
   "source": [
    "test_dataset = SegmentationDataset(split=\"test\", preload=True, data_dir=DATASET_PATH, transform=False)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2, drop_last=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mp4_shared.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}